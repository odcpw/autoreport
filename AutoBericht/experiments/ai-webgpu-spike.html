<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>AI WebGPU Spike</title>
    <link rel="stylesheet" href="ai-webgpu-spike.css" />
  </head>
  <body>
    <header class="page-header">
      <div>
        <h1>AI WebGPU Spike</h1>
        <p>Quick browser test for Transformers.js (ASR + vision) with optional WebGPU.</p>
      </div>
      <div class="badge">experiments</div>
    </header>

    <main>
      <section class="card" id="download-section">
        <h2>Offline downloads (local-only)</h2>
        <p class="hint">
          Download files once, save them under <code>AutoBericht/experiments/</code>,
          then serve this folder via the local server. Model IDs stay the same, but
          the files are loaded from <code>/AutoBericht/experiments/models/</code>.
        </p>

        <div class="download-block">
          <h3>Transformers.js (ESM bundle)</h3>
          <p class="hint">Save into <code>AutoBericht/experiments/vendor/transformers.min.js</code>.</p>
          <ul class="download-list">
            <li>
              <a href="https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.8.0" target="_blank" rel="noreferrer">
                Download @huggingface/transformers@3.8.0 (ESM entry)
              </a>
            </li>
          </ul>
        </div>

        <div class="download-block">
          <h3>onnxruntime-web (WebGPU bundle)</h3>
          <p class="hint">Save into <code>AutoBericht/experiments/vendor/ort.webgpu.min.js</code>.</p>
          <ul class="download-list">
            <li>
              <a href="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.webgpu.min.js" target="_blank" rel="noreferrer">
                Download onnxruntime-web@1.18.0 (ort.webgpu.min.js)
              </a>
            </li>
          </ul>
          <p class="hint">WASM runtime files are included in the repo under <code>vendor/</code>.</p>
        </div>

        <div class="download-block">
          <h3>Local folder structure (pre-made)</h3>
          <pre class="folder-structure">AutoBericht/experiments/
├── vendor/
│   └── transformers.min.js
│   └── ort.webgpu.min.js
└── models/
    ├── Xenova/
    │   └── whisper-tiny.en/
    │       ├── onnx/
    │       └── *.json + tokenizer files
    └── LiquidAI/
        └── LFM2.5-VL-1.6B-ONNX/
            ├── onnx/
            └── *.json + tokenizer files
</pre>
        </div>

        <details class="download-block">
          <summary>Whisper model sizes to compare (ASR)</summary>
          <p class="hint">
            Use the same file pattern as tiny, just in a different model folder.
            Smaller models load faster but may be less accurate.
          </p>
          <ul class="download-list">
            <li>
              <strong>tiny.en</strong> (fastest, lowest quality)
              — <a href="https://huggingface.co/Xenova/whisper-tiny.en/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
            <li>
              <strong>base.en</strong> (balanced)
              — <a href="https://huggingface.co/Xenova/whisper-base.en/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
            <li>
              <strong>small.en</strong> (better quality, slower)
              — <a href="https://huggingface.co/Xenova/whisper-small.en/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
          </ul>
        </details>

        <details class="download-block">
          <summary>Whisper tiny.en (ASR, q4 WebGPU set) — large files only</summary>
          <p class="hint">
            Small config/tokenizer files are already in the repo.
            Download only these ONNX files into <code>AutoBericht/experiments/models/Xenova/whisper-tiny.en/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/Xenova/whisper-tiny.en/resolve/main/onnx/encoder_model_q4.onnx" target="_blank" rel="noreferrer">onnx/encoder_model_q4.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-tiny.en/resolve/main/onnx/decoder_model_q4.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_q4.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-tiny.en/resolve/main/onnx/decoder_with_past_model_q4.onnx" target="_blank" rel="noreferrer">onnx/decoder_with_past_model_q4.onnx</a></li>
          </ul>
          <p class="hint">These are the only large files needed for the q4 dtype in the spike page.</p>
        </details>

        <details class="download-block">
          <summary>LiquidAI vision models (WebGPU ONNX)</summary>
          <p class="hint">
            Official WebGPU-ready ONNX export (LFM2.5‑VL‑1.6B) plus a smaller community ONNX export (450M).
          </p>
          <ul class="download-list">
            <li>
              <strong>LFM2.5‑VL‑1.6B‑ONNX</strong> (official, WebGPU notes included)
              — <a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX" target="_blank" rel="noreferrer">HF repo</a>
            </li>
            <li>
              <strong>LFM2‑VL‑450M‑ONNX</strong> (community, lighter)
              — <a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX" target="_blank" rel="noreferrer">HF repo</a>
            </li>
          </ul>
        </details>

        <details class="download-block">
          <summary>LiquidAI LFM2.5‑VL‑1.6B‑ONNX — large files only</summary>
          <p class="hint">
            Small config/tokenizer files are already in the repo.
            Download these ONNX files into <code>AutoBericht/experiments/models/LiquidAI/LFM2.5-VL-1.6B-ONNX/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_tokens_fp16.onnx" target="_blank" rel="noreferrer">onnx/embed_tokens_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_tokens_fp16.onnx_data" target="_blank" rel="noreferrer">onnx/embed_tokens_fp16.onnx_data</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_images_fp16.onnx" target="_blank" rel="noreferrer">onnx/embed_images_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_images_fp16.onnx_data" target="_blank" rel="noreferrer">onnx/embed_images_fp16.onnx_data</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/decoder_q4.onnx" target="_blank" rel="noreferrer">onnx/decoder_q4.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/decoder_q4.onnx_data" target="_blank" rel="noreferrer">onnx/decoder_q4.onnx_data</a></li>
          </ul>
        </details>

        <details class="download-block">
          <summary>LiquidAI LFM2‑VL‑450M‑ONNX — large files only (community)</summary>
          <p class="hint">
            Small config/tokenizer files are already in the repo.
            Download these ONNX files into <code>AutoBericht/experiments/models/LiquidAI/LFM2-VL-450M-ONNX/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/vision_encoder.onnx" target="_blank" rel="noreferrer">onnx/vision_encoder.onnx</a></li>
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/vision_encoder.onnx_data" target="_blank" rel="noreferrer">onnx/vision_encoder.onnx_data</a></li>
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/embed_tokens.onnx" target="_blank" rel="noreferrer">onnx/embed_tokens.onnx</a></li>
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/embed_tokens.onnx_data" target="_blank" rel="noreferrer">onnx/embed_tokens.onnx_data</a></li>
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/decoder_model_merged.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_merged.onnx</a></li>
            <li><a href="https://huggingface.co/onnx-community/LFM2-VL-450M-ONNX/resolve/main/onnx/decoder_model_merged.onnx_data" target="_blank" rel="noreferrer">onnx/decoder_model_merged.onnx_data</a></li>
          </ul>
        </details>
      </section>

      <section class="card" id="env-section">
        <h2>Environment</h2>
        <p class="hint">
          Open via <strong>http://localhost</strong> to enable WebGPU and file access.
        </p>
        <div class="grid">
          <label>
            Transformers.js URL
            <input id="transformers-url" type="text" spellcheck="false" placeholder="Paste a module URL (local or CDN)" value="./vendor/transformers.min.js" />
          </label>
          <label>
            Local model path (optional)
            <input id="local-model-path" type="text" placeholder="/models/" value="/AutoBericht/experiments/models/" />
          </label>
          <label class="inline">
            <input id="allow-remote" type="checkbox" />
            Allow remote models (HF Hub)
          </label>
          <label>
            Device
            <select id="device-select">
              <option value="auto" selected>auto</option>
              <option value="webgpu">webgpu</option>
              <option value="wasm">wasm</option>
            </select>
          </label>
        </div>
        <div class="actions">
          <button id="load-lib">Load library</button>
          <button id="check-webgpu">Check WebGPU</button>
        </div>
        <div class="status" id="env-status">Library not loaded.</div>
      </section>

      <section class="card" id="asr-section">
        <h2>Speech → Text (Whisper)</h2>
        <div class="grid">
          <label>
            ASR model id
            <input id="asr-model" type="text" spellcheck="false" placeholder="Model id for ASR" value="Xenova/whisper-tiny.en" />
            <span class="hint">Using q4 dtype (matches the download list).</span>
          </label>
          <label>
            Audio file
            <input id="asr-file" type="file" accept="audio/*" />
          </label>
          <label class="inline">
            <input id="asr-timestamps" type="checkbox" />
            Return word timestamps
          </label>
        </div>
        <div class="actions">
          <button id="run-asr">Transcribe</button>
        </div>
        <div class="status" id="asr-status">Awaiting input.</div>
        <textarea id="asr-output" rows="8" spellcheck="false" placeholder="Transcript output..."></textarea>
      </section>

      <section class="card" id="vision-section">
        <h2>Vision (Image → Text)</h2>
        <div class="grid">
          <label>
            Vision task
            <select id="vision-task">
              <option value="image-to-text" selected>image-to-text</option>
              <option value="image-classification">image-classification</option>
            </select>
          </label>
          <label>
            Vision model id
            <input id="vision-model" type="text" spellcheck="false" placeholder="Model id for vision" value="LiquidAI/LFM2.5-VL-1.6B-ONNX" />
            <span class="hint">LiquidAI ONNX models run embedding tests via onnxruntime-web.</span>
          </label>
          <label>
            Question about image (LiquidAI only)
            <input id="vision-question" type="text" spellcheck="false" placeholder="Ask a question about the image..." />
          </label>
          <label>
            Image file
            <input id="vision-file" type="file" accept="image/*" />
          </label>
        </div>
        <div class="actions">
          <button id="run-vision">Analyze image</button>
          <button id="ask-vision">Ask (LiquidAI)</button>
        </div>
        <div class="status" id="vision-status">Awaiting input.</div>
        <div class="vision-preview">
          <img id="vision-preview" alt="" />
          <pre id="vision-output"></pre>
        </div>
      </section>

      <section class="card" id="log-section">
        <h2>Log</h2>
        <pre id="log"></pre>
      </section>
    </main>

    <script src="./vendor/ort.webgpu.min.js"></script>
    <script type="module" src="ai-webgpu-spike.js"></script>
  </body>
</html>
