<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>AI WebGPU Spike</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <header class="page-header">
      <div>
        <h1>AI WebGPU Spike</h1>
        <p>Quick browser test for Transformers.js (ASR + vision) with optional WebGPU.</p>
      </div>
      <div class="badge">experiments</div>
    </header>

    <main>
      <section class="card" id="download-section">
        <h2>Offline assets (local-only)</h2>
        <p class="hint">
          The spike is configured to load everything from <code>AutoBericht/AI/</code>. No external downloads or CDNs
          are required when using the local bundle.
        </p>

        <div class="download-block">
          <h3>Local folder structure (pre-made)</h3>
          <pre class="folder-structure">AutoBericht/AI/
├── vendor/
│   ├── transformers.min.js
│   └── ort-1.23.2/
│       ├── ort.webgpu.min.js
│       └── ort-wasm-*.wasm
└── models/
    ├── Xenova/
    │   ├── whisper-tiny/
    │   │   ├── onnx/
    │   │   └── *.json + tokenizer files
    │   └── whisper-base/
    │       ├── onnx/
    │       └── *.json + tokenizer files
    └── LiquidAI/
        └── LFM2.5-VL-1.6B-ONNX/
            ├── onnx/
            └── *.json + tokenizer files
</pre>
        </div>

        <details class="download-block">
          <summary>Whisper model sizes to compare (ASR, multilingual)</summary>
          <p class="hint">
            Use the same file pattern as tiny, just in a different model folder.
            Smaller models load faster but may be less accurate.
          </p>
          <ul class="download-list">
            <li>
              <strong>tiny</strong> (fastest, lowest quality)
              — <a href="https://huggingface.co/Xenova/whisper-tiny/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
            <li>
              <strong>base</strong> (balanced)
              — <a href="https://huggingface.co/Xenova/whisper-base/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
            <li>
              <strong>small</strong> (better quality, slower)
              — <a href="https://huggingface.co/Xenova/whisper-small/tree/main" target="_blank" rel="noreferrer">HF repo</a>
            </li>
          </ul>
        </details>

        <details class="download-block">
          <summary>Whisper tiny (ASR, fp16 WebGPU set) — large files only</summary>
          <p class="hint">
            Small config/tokenizer files live alongside the ONNX files.
            Download these ONNX files into <code>AutoBericht/AI/models/Xenova/whisper-tiny/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/Xenova/whisper-tiny/resolve/main/onnx/encoder_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/encoder_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-tiny/resolve/main/onnx/decoder_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-tiny/resolve/main/onnx/decoder_with_past_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_with_past_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-tiny/resolve/main/onnx/decoder_model_merged_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_merged_fp16.onnx</a></li>
          </ul>
          <p class="hint">These are the only large files needed for the fp16 dtype in the spike page.</p>
          <p class="hint">If your WebGPU adapter lacks <code>shader-f16</code>, use fp32 files instead: <code>encoder_model.onnx</code>, <code>decoder_model.onnx</code>, <code>decoder_with_past_model.onnx</code>, <code>decoder_model_merged.onnx</code>.</p>
        </details>

        <details class="download-block">
          <summary>Whisper base (ASR, fp16 WebGPU set) — large files only</summary>
          <p class="hint">
            Download these ONNX files into <code>AutoBericht/AI/models/Xenova/whisper-base/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/Xenova/whisper-base/resolve/main/onnx/encoder_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/encoder_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-base/resolve/main/onnx/decoder_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-base/resolve/main/onnx/decoder_with_past_model_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_with_past_model_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/Xenova/whisper-base/resolve/main/onnx/decoder_model_merged_fp16.onnx" target="_blank" rel="noreferrer">onnx/decoder_model_merged_fp16.onnx</a></li>
          </ul>
        </details>

        <details class="download-block">
          <summary>LiquidAI vision models (WebGPU ONNX)</summary>
          <p class="hint">
            Official WebGPU-ready ONNX export (LFM2.5‑VL‑1.6B).
          </p>
          <ul class="download-list">
            <li>
              <strong>LFM2.5‑VL‑1.6B‑ONNX</strong> (official, WebGPU notes included)
              — <a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX" target="_blank" rel="noreferrer">HF repo</a>
            </li>
          </ul>
        </details>

        <details class="download-block">
          <summary>LiquidAI LFM2.5‑VL‑1.6B‑ONNX — large files only</summary>
          <p class="hint">
            Small config/tokenizer files are already in the repo.
            Download these ONNX files into <code>AutoBericht/AI/models/LiquidAI/LFM2.5-VL-1.6B-ONNX/onnx/</code>.
          </p>
          <ul class="download-list">
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_tokens_fp16.onnx" target="_blank" rel="noreferrer">onnx/embed_tokens_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_tokens_fp16.onnx_data" target="_blank" rel="noreferrer">onnx/embed_tokens_fp16.onnx_data</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_images_fp16.onnx" target="_blank" rel="noreferrer">onnx/embed_images_fp16.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/embed_images_fp16.onnx_data" target="_blank" rel="noreferrer">onnx/embed_images_fp16.onnx_data</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/decoder_q4.onnx" target="_blank" rel="noreferrer">onnx/decoder_q4.onnx</a></li>
            <li><a href="https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-ONNX/resolve/main/onnx/decoder_q4.onnx_data" target="_blank" rel="noreferrer">onnx/decoder_q4.onnx_data</a></li>
          </ul>
        </details>

      </section>

      <section class="card" id="env-section">
        <h2>Environment</h2>
        <p class="hint">
          Open via <strong>http://localhost</strong> to enable WebGPU and file access.
        </p>
        <div class="grid">
          <label>
            Transformers.js URL (local)
            <input id="transformers-url" type="text" spellcheck="false" placeholder="Local module path" value="../../AI/vendor/transformers.min.js" />
          </label>
          <label>
            Local model path (optional)
            <input id="local-model-path" type="text" placeholder="../../AI/models/" value="../../AI/models/" />
          </label>
          <label class="inline">
            <input id="allow-remote" type="checkbox" />
            Allow remote models (HF Hub)
          </label>
          <label class="inline">
            <input id="disable-wasm-simd" type="checkbox" />
            Disable WASM SIMD (debug)
          </label>
          <label>
            ORT version
            <select id="ort-version">
              <option value="1.23.2">1.23.2 (Space)</option>
            </select>
            <span class="hint">Reloads the page. 1.23.2 loads from <code>AI/vendor/ort-1.23.2/</code>.</span>
          </label>
          <label>
            ORT bundle
            <select id="ort-bundle">
              <option value="webgpu">webgpu</option>
              <option value="wasm">wasm-only</option>
            </select>
            <span class="hint">Reloads the page.</span>
          </label>
          <label>
            Device
            <select id="device-select">
              <option value="auto" selected>auto</option>
              <option value="webgpu">webgpu</option>
              <option value="wasm">wasm</option>
            </select>
          </label>
        </div>
        <div class="actions">
          <button id="load-lib">Load library</button>
          <button id="check-webgpu">Check WebGPU</button>
          <button id="probe-wasm">Probe WASM files</button>
          <button id="report-webgpu">GPU report</button>
        </div>
        <div class="status" id="env-status">Library not loaded.</div>
      </section>

      <section class="card" id="asr-section">
        <h2>Speech → Text (Whisper)</h2>
        <div class="grid">
          <label>
            ASR model id
            <input id="asr-model" type="text" spellcheck="false" placeholder="Model id for ASR" value="Xenova/whisper-tiny" />
            <span class="hint">Prefers fp16 for WebGPU; falls back to fp32 if shader-f16 is unavailable.</span>
          </label>
          <label>
            Audio file
            <input id="asr-file" type="file" accept="audio/*" />
          </label>
          <label class="inline">
            <input id="asr-timestamps" type="checkbox" />
            Return word timestamps
          </label>
        </div>
        <div class="actions">
          <button id="run-asr">Transcribe</button>
        </div>
        <div class="status" id="asr-status">Awaiting input.</div>
        <textarea id="asr-output" rows="8" spellcheck="false" placeholder="Transcript output..."></textarea>
      </section>

      <section class="card" id="vision-section">
        <h2>Vision (Image → Text)</h2>
        <div class="grid">
          <label>
            Vision task
            <select id="vision-task">
              <option value="image-to-text" selected>image-to-text</option>
              <option value="image-classification">image-classification</option>
            </select>
          </label>
          <label>
            Vision model id
            <input id="vision-model" type="text" spellcheck="false" placeholder="Model id for vision" value="LiquidAI/LFM2.5-VL-1.6B-ONNX" />
            <span class="hint">LiquidAI ONNX models run embedding tests via onnxruntime-web.</span>
          </label>
          <label>
            Question about image (LiquidAI only)
            <input id="vision-question" type="text" spellcheck="false" placeholder="Ask a question about the image..." />
          </label>
          <label>
            Image file
            <input id="vision-file" type="file" accept="image/*" />
          </label>
        </div>
        <div class="actions">
          <button id="run-vision">Analyze image</button>
          <button id="load-vision-onnx">Load ONNX sessions</button>
          <button id="load-vision-onnx-wasm">Load ONNX (WASM)</button>
          <button id="load-vision-onnx-webgpu">Load ONNX (WebGPU)</button>
          <button id="ask-vision">Ask (LiquidAI)</button>
        </div>
        <div class="status" id="vision-status">Awaiting input.</div>
        <div class="vision-preview">
          <img id="vision-preview" alt="" />
          <pre id="vision-output"></pre>
        </div>
      </section>

      <section class="card" id="log-section">
        <h2>Log</h2>
        <div class="actions">
          <button id="download-log">Download log</button>
        </div>
        <pre id="log"></pre>
      </section>
    </main>

    <script>
      (function () {
        const params = new URLSearchParams(window.location.search);
        const ortVersion = params.get("ort") || "1.23.2";
        const ortBundle = params.get("ortbundle") || "webgpu";
        const base = ortVersion === "1.23.2"
          ? "../../AI/vendor/ort-1.23.2/"
          : "../../AI/vendor/ort-1.23.2/";
        const scriptName = ortBundle === "wasm" ? "ort.min.js" : "ort.webgpu.min.js";
        window.__ortConfig = { version: ortVersion, base, bundle: ortBundle };
        window.__ortLoadPromise = new Promise((resolve, reject) => {
          const script = document.createElement("script");
          script.src = `${base}${scriptName}?v=${ortVersion}`;
          script.async = true;
          script.onload = () => resolve();
          script.onerror = () => reject(new Error("Failed to load ORT script."));
          document.head.appendChild(script);
        });
      })();
    </script>
    <script type="module" src="app.js"></script>
  </body>
</html>
